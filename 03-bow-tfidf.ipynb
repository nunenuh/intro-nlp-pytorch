{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-Words and TF-IDF representations\n",
        "\n",
        "In the previous unit, we have learnt to represent text by numbers. In this unit, we'll explore some of the approaches to feeding variable-length text into a neural network to collapse the input sequence into a fixed-length vector, which can then be used in the classifier.\n",
        "\n",
        "To begin with, let's load our AG News dataset and build the vocabulary, we we have done in the previous unit. To make things shorter, all those operations are combined into `load_dataset` function of the accompanying Python module:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt\n",
        "!wget -q https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/torchnlp.py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "from torchnlp import *\n",
        "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size = \",vocab_size)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading dataset...\nBuilding vocab...\nVocab size =  95812\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words text representation\n",
        "\n",
        "Because words represent meaning, sometimes we can figure out the **_meaning of a text_** by just looking at the individual words, regardless of their order in the sentence. For example, when classifying news, words like *weather*, *snow* are likely to indicate *weather forecast*, while words like *stocks*, *dollar* would count towards *financial news*.\n",
        "\n",
        "**Bag of Words** (BoW) vector representation is the most commonly used traditional vector representation. Each word is linked to a vector index, vector element contains the number of occurrences of a word in a given document.\n",
        "\n",
        "<img alt=\"Image showing how a bag of words vector representation is represented in memory.\" src=\"images/3-bow-tfidf-1.png\" align=\"middle\" />\n",
        "\n",
        "> **Note**: You can also think of BoW as a sum of all one-hot-encoded vectors for individual words in the text.\n",
        "\n",
        "Below is an example of how to generate a bag of word representation for a text using vectorization defined previously:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def to_bow(text,bow_vocab_size=vocab_size):\n",
        "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
        "    for i in encode(text):\n",
        "        if i<bow_vocab_size:\n",
        "            res[i] += 1\n",
        "    return res\n",
        "\n",
        "print(f\"sample text:\\n{train_dataset[0][1]}\")\n",
        "print(f\"\\nBoW vector:\\n{to_bow(train_dataset[0][1])}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "sample text:\nWall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n\nBoW vector:\ntensor([0., 0., 2.,  ..., 0., 0., 0.])\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** Here we are using global `vocab_size` variable to specify default size of the vocabulary. Since often vocabulary size are pretty big, we can limit the size of the vocabulary to most frequent words. Try lowering `vocab_size` value and running the code below, and see how it affects the accuracy. You should expect some accuracy drop, but not dramatic, in lieu of higher performance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training BoW classifier\n",
        "\n",
        "Now that we have learned how to build a Bag-of-Words representation of our text, let's train a classifier on top of it. First, we need to convert our dataset for training in such a way, that all positional vector representations are converted to bag-of-words representation. This can be achieved by passing `bowify` function as `collate_fn` parameter to standard torch `DataLoader`.  The `collate_fn` gives you the ability to apply your own function to the dataset as it's loaded by the Dataloader:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np \n",
        "\n",
        "# this collate function gets list of batch_size tuples, and needs to \n",
        "# return a pair of label-feature tensors for the whole minibatch\n",
        "def bowify(b):\n",
        "    return (\n",
        "            torch.LongTensor([t[0]-1 for t in b]),\n",
        "            torch.stack([to_bow(t[1]) for t in b])\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define a simple classifier neural network that contains one linear layer. The size of the input vector equals to `vocab_size`, and the output size corresponds to the number of classes (4). Because we are solving a classification task, the final activation function is `LogSoftmax()`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will define a standard PyTorch training loop. Because our dataset is quite large, for our teaching purpose we will train only for one epoch, and sometimes even for less than an epoch (specifying the `epoch_size` parameter allows us to limit training). We would also report accumulated training accuracy during training; the frequency of reporting is specified using `report_freq` parameter."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    net.train()\n",
        "    total_loss,acc,count,i = 0,0,0,0\n",
        "    for labels,features in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        out = net(features)\n",
        "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss+=loss\n",
        "        _,predicted = torch.max(out,1)\n",
        "        acc+=(predicted==labels).sum()\n",
        "        count+=len(labels)\n",
        "        i+=1\n",
        "        if i%report_freq==0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "        if epoch_size and count>epoch_size:\n",
        "            break\n",
        "    return total_loss.item()/count, acc.item()/count"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the classifier performs on the training dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch(net,train_loader,epoch_size=15000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3200: acc=0.80125\n6400: acc=0.83984375\n9600: acc=0.8560416666666667\n12800: acc=0.8628125\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "(0.025899573938170477, 0.8665378464818764)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The Bag-of-Words approach can be used in the same manner with n-gram tokenizer - only that the vocabulary size would be bigger, and thus the network would have too many parameters. In the next unit, we will see how bigram representation can be used together with embeddings."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Term Frequency / Inverse Document Frequency:  TF-IDF\n",
        "\n",
        "In BoW representation, word occurrences are evenly weighted, regardless of the word itself. However, it is clear that frequent words, such as *'a'*, *'in'*, *'the'* etc. are much less important for the classification, than specialized terms. In fact, in most NLP tasks some words are more relevant than others.\n",
        "\n",
        "**TF-IDF** stands for **term frequency–inverse document frequency**. It is a variation of bag of words, where instead of a binary 0/1 value indicating the appearance of a word in a document, a floating-point value is used, which is related to the **_frequency of word occurrence_** in the corpus.\n",
        "\n",
        "The formula to calculate TF-IDF is:  $w_{ij} = tf_{ij}\\times\\log({N\\over df_i})$\n",
        "\n",
        "Here's the meaning of each parameter in the formula:\n",
        "* $i$ is the word \n",
        "* $j$ is the document\n",
        "* $w_{ij}$ is the weight or the importance of the word in the document\n",
        "* $tf_{ij}$ is the number of occurrences of the word $i$ in the document $j$, i.e. the BoW value we have seen before\n",
        "* $N$ is the number of documents in the collection\n",
        "* $df_i$ is the number of documents containing the word $i$ in the whole collection\n",
        "\n",
        "<img alt=\"Diagram showing table representing word frequent in documents.\" src=\"images/3-bow-tfidf-2.png\" align=\"middle\" />\n",
        "\n",
        "TF-IDF value $w_{ij}$ increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contains the word, which helps to adjust for the fact that some words appear more frequently than others. For example, if the word appears in *every* document in the collection, $df_i=N$, and $w_{ij}=0$, and those terms would be completely disregarded.\n",
        "\n",
        "First, let's compute document frequency $df_i$ for each word $i$. We can represent it as tensor of size `vocab_size`. We will limit the number of documents to $N=1000$ to speed up processing. For each input sentence, we compute the set of words (represented by their numbers), and increase the corresponding counter:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "N = 1000\n",
        "df = torch.zeros(vocab_size)\n",
        "for _,line in train_dataset[:N]:\n",
        "    for i in set(encode(line)):\n",
        "        df[i] += 1"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have document frequencies for each word, we can define `tf_idf` function that will take a string, and produce TF-IDF vector. We will use `to_bow` defined above to calculate term frequency vector, and multiply it by inverse document frequency of the corresponding term. Remember that all tensor operations are element-wise, which allows us to implement the whole computation as a tensor formula:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf(s):\n",
        "    bow = to_bow(s)\n",
        "    return bow*torch.log((N+1)/(df+1))\n",
        "\n",
        "print(tf_idf(train_dataset[0][1]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([0.0000, 0.0000, 0.0363,  ..., 0.0000, 0.0000, 0.0000])\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> You may have noticed that we used a slightly different formula for TF-IDF, namely $\\log({N+1\\over df_i+1})$ instead of $\\log({N\\over df_i})$. This yields similar results, but prevents division by 0 in those cases when $df_i=0$.\n",
        "\n",
        "Even though TF-IDF representation calculates different weights to different words according to their importance, it is unable to correctly capture the meaning, largely because the order of words in the sentence is still not taken into account. As the famous linguist J. R. Firth said in 1935, “The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously”. We will learn in the later units how to capture contextual information from text using language modeling.\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "0cb620c6d4b9f7a635928804c26cf22403d89d98d79684e4529119355ee6d5a5"
    },
    "kernelspec": {
      "display_name": "azureml_py38_PT_and_TF",
      "language": "python",
      "name": "conda-env-azureml_py38_PT_and_TF-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "azureml_py38_PT_and_TF",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}